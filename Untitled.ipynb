{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc6f7cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5022b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [str(x) for x in Path(\".\").glob(\"**/*.tokens\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f2fd28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data\\\\wikitext-2\\\\wiki.test.tokens',\n",
       " 'data\\\\wikitext-2\\\\wiki.train.tokens',\n",
       " 'data\\\\wikitext-2\\\\wiki.valid.tokens']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "705c205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f02da1f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wikiTransformer\\\\vocab.json', 'wikiTransformer\\\\merges.txt']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!mkdir wikiTransformer\n",
    "tokenizer.save_model(\"wikiTransformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbfe36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./wikiTransformer/vocab.json\",\n",
    "    \"./wikiTransformer/merges.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fad93822",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eabca1a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'ad', 'vent', 'Ä of', 'Ä nuclear', 'Ä weapons', '</s>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"advent of nuclear weapons\").tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f725ea05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4adfd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./wikiTransformer\", max_len=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a98f1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_model import make_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07442709",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_model(52000,52000,6,512,2048,8,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d40634a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 191 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"./data/wikitext-2/wiki.test.tokens\",\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ee21ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3c76e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./wikiTransformer\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d0f0e14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Trainer in module transformers.trainer:\n",
      "\n",
      "class Trainer(builtins.object)\n",
      " |  Trainer(model: Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module] = None, args: transformers.training_args.TrainingArguments = None, data_collator: Optional[DataCollator] = None, train_dataset: Optional[torch.utils.data.dataset.Dataset] = None, eval_dataset: Optional[torch.utils.data.dataset.Dataset] = None, tokenizer: Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase] = None, model_init: Callable[[], transformers.modeling_utils.PreTrainedModel] = None, compute_metrics: Optional[Callable[[transformers.trainer_utils.EvalPrediction], Dict]] = None, callbacks: Optional[List[transformers.trainer_callback.TrainerCallback]] = None, optimizers: Tuple[torch.optim.optimizer.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None), preprocess_logits_for_metrics: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = None)\n",
      " |  \n",
      " |  Trainer is a simple but feature-complete training and eval loop for PyTorch, optimized for ðŸ¤— Transformers.\n",
      " |  \n",
      " |  Args:\n",
      " |      model ([`PreTrainedModel`] or `torch.nn.Module`, *optional*):\n",
      " |          The model to train, evaluate or use for predictions. If not provided, a `model_init` must be passed.\n",
      " |  \n",
      " |          <Tip>\n",
      " |  \n",
      " |          [`Trainer`] is optimized to work with the [`PreTrainedModel`] provided by the library. You can still use\n",
      " |          your own models defined as `torch.nn.Module` as long as they work the same way as the ðŸ¤— Transformers\n",
      " |          models.\n",
      " |  \n",
      " |          </Tip>\n",
      " |  \n",
      " |      args ([`TrainingArguments`], *optional*):\n",
      " |          The arguments to tweak for training. Will default to a basic instance of [`TrainingArguments`] with the\n",
      " |          `output_dir` set to a directory named *tmp_trainer* in the current directory if not provided.\n",
      " |      data_collator (`DataCollator`, *optional*):\n",
      " |          The function to use to form a batch from a list of elements of `train_dataset` or `eval_dataset`. Will\n",
      " |          default to [`default_data_collator`] if no `tokenizer` is provided, an instance of\n",
      " |          [`DataCollatorWithPadding`] otherwise.\n",
      " |      train_dataset (`torch.utils.data.Dataset` or `torch.utils.data.IterableDataset`, *optional*):\n",
      " |          The dataset to use for training. If it is an `datasets.Dataset`, columns not accepted by the\n",
      " |          `model.forward()` method are automatically removed.\n",
      " |  \n",
      " |          Note that if it's a `torch.utils.data.IterableDataset` with some randomization and you are training in a\n",
      " |          distributed fashion, your iterable dataset should either use a internal attribute `generator` that is a\n",
      " |          `torch.Generator` for the randomization that must be identical on all processes (and the Trainer will\n",
      " |          manually set the seed of this `generator` at each epoch) or have a `set_epoch()` method that internally\n",
      " |          sets the seed of the RNGs used.\n",
      " |      eval_dataset (`torch.utils.data.Dataset`, *optional*):\n",
      " |           The dataset to use for evaluation. If it is an `datasets.Dataset`, columns not accepted by the\n",
      " |           `model.forward()` method are automatically removed.\n",
      " |      tokenizer ([`PreTrainedTokenizerBase`], *optional*):\n",
      " |          The tokenizer used to preprocess the data. If provided, will be used to automatically pad the inputs the\n",
      " |          maximum length when batching inputs, and it will be saved along the model to make it easier to rerun an\n",
      " |          interrupted training or reuse the fine-tuned model.\n",
      " |      model_init (`Callable[[], PreTrainedModel]`, *optional*):\n",
      " |          A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start\n",
      " |          from a new instance of the model as given by this function.\n",
      " |  \n",
      " |          The function may have zero argument, or a single one containing the optuna/Ray Tune/SigOpt trial object, to\n",
      " |          be able to choose different architectures according to hyper parameters (such as layer count, sizes of\n",
      " |          inner layers, dropout probabilities etc).\n",
      " |      compute_metrics (`Callable[[EvalPrediction], Dict]`, *optional*):\n",
      " |          The function that will be used to compute metrics at evaluation. Must take a [`EvalPrediction`] and return\n",
      " |          a dictionary string to metric values.\n",
      " |      callbacks (List of [`TrainerCallback`], *optional*):\n",
      " |          A list of callbacks to customize the training loop. Will add those to the list of default callbacks\n",
      " |          detailed in [here](callback).\n",
      " |  \n",
      " |          If you want to remove one of the default callbacks used, use the [`Trainer.remove_callback`] method.\n",
      " |      optimizers (`Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR]`, *optional*): A tuple\n",
      " |          containing the optimizer and the scheduler to use. Will default to an instance of [`AdamW`] on your model\n",
      " |          and a scheduler given by [`get_linear_schedule_with_warmup`] controlled by `args`.\n",
      " |      preprocess_logits_for_metrics (`Callable[[torch.Tensor, torch.Tensor], torch.Tensor]`, *optional*):\n",
      " |          A function that preprocess the logits right before caching them at each evaluation step. Must take two\n",
      " |          tensors, the logits and the labels, and return the logits once processed as desired. The modifications made\n",
      " |          by this function will be reflected in the predictions received by `compute_metrics`.\n",
      " |  \n",
      " |          Note that the labels (second parameter) will be `None` if the dataset does not have them.\n",
      " |  \n",
      " |  Important attributes:\n",
      " |  \n",
      " |      - **model** -- Always points to the core model. If using a transformers model, it will be a [`PreTrainedModel`]\n",
      " |        subclass.\n",
      " |      - **model_wrapped** -- Always points to the most external model in case one or more other modules wrap the\n",
      " |        original model. This is the model that should be used for the forward pass. For example, under `DeepSpeed`,\n",
      " |        the inner model is wrapped in `DeepSpeed` and then again in `torch.nn.DistributedDataParallel`. If the inner\n",
      " |        model hasn't been wrapped, then `self.model_wrapped` is the same as `self.model`.\n",
      " |      - **is_model_parallel** -- Whether or not a model has been switched to a model parallel mode (different from\n",
      " |        data parallelism, this means some of the model layers are split on different GPUs).\n",
      " |      - **place_model_on_device** -- Whether or not to automatically place the model on the device - it will be set\n",
      " |        to `False` if model parallel or deepspeed is used, or if the default\n",
      " |        `TrainingArguments.place_model_on_device` is overridden to return `False` .\n",
      " |      - **is_in_train** -- Whether or not a model is currently running `train` (e.g. when `evaluate` is called while\n",
      " |        in `train`)\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, model: Union[transformers.modeling_utils.PreTrainedModel, torch.nn.modules.module.Module] = None, args: transformers.training_args.TrainingArguments = None, data_collator: Optional[DataCollator] = None, train_dataset: Optional[torch.utils.data.dataset.Dataset] = None, eval_dataset: Optional[torch.utils.data.dataset.Dataset] = None, tokenizer: Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase] = None, model_init: Callable[[], transformers.modeling_utils.PreTrainedModel] = None, compute_metrics: Optional[Callable[[transformers.trainer_utils.EvalPrediction], Dict]] = None, callbacks: Optional[List[transformers.trainer_callback.TrainerCallback]] = None, optimizers: Tuple[torch.optim.optimizer.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None), preprocess_logits_for_metrics: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  add_callback(self, callback)\n",
      " |      Add a callback to the current list of [`~transformer.TrainerCallback`].\n",
      " |      \n",
      " |      Args:\n",
      " |         callback (`type` or [`~transformer.TrainerCallback`]):\n",
      " |             A [`~transformer.TrainerCallback`] class or an instance of a [`~transformer.TrainerCallback`]. In the\n",
      " |             first case, will instantiate a member of that class.\n",
      " |  \n",
      " |  autocast_smart_context_manager(self)\n",
      " |      A helper wrapper that creates an appropriate context manager for `autocast` while feeding it the desired\n",
      " |      arguments, depending on the situation.\n",
      " |  \n",
      " |  call_model_init(self, trial=None)\n",
      " |  \n",
      " |  compute_loss(self, model, inputs, return_outputs=False)\n",
      " |      How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
      " |      \n",
      " |      Subclass and override for custom behavior.\n",
      " |  \n",
      " |  create_model_card(self, language: Optional[str] = None, license: Optional[str] = None, tags: Optional[str] = None, model_name: Optional[str] = None, finetuned_from: Optional[str] = None, tasks: Optional[str] = None, dataset_tags: Union[List[str], str, NoneType] = None, dataset: Union[List[str], str, NoneType] = None, dataset_args: Union[List[str], str, NoneType] = None)\n",
      " |  \n",
      " |  create_optimizer(self)\n",
      " |      Setup the optimizer.\n",
      " |      \n",
      " |      We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
      " |      Trainer's init through `optimizers`, or subclass and override this method in a subclass.\n",
      " |  \n",
      " |  create_optimizer_and_scheduler(self, num_training_steps: int)\n",
      " |      Setup the optimizer and the learning rate scheduler.\n",
      " |      \n",
      " |      We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
      " |      Trainer's init through `optimizers`, or subclass and override this method (or `create_optimizer` and/or\n",
      " |      `create_scheduler`) in a subclass.\n",
      " |  \n",
      " |  create_scheduler(self, num_training_steps: int, optimizer: torch.optim.optimizer.Optimizer = None)\n",
      " |      Setup the scheduler. The optimizer of the trainer must have been set up either before this method is called or\n",
      " |      passed as an argument.\n",
      " |      \n",
      " |      Args:\n",
      " |          num_training_steps (int): The number of training steps to do.\n",
      " |  \n",
      " |  evaluate(self, eval_dataset: Optional[torch.utils.data.dataset.Dataset] = None, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = 'eval') -> Dict[str, float]\n",
      " |      Run evaluation and returns metrics.\n",
      " |      \n",
      " |      The calling script will be responsible for providing a method to compute metrics, as they are task-dependent\n",
      " |      (pass it to the init `compute_metrics` argument).\n",
      " |      \n",
      " |      You can also subclass and override this method to inject custom behavior.\n",
      " |      \n",
      " |      Args:\n",
      " |          eval_dataset (`Dataset`, *optional*):\n",
      " |              Pass a dataset if you wish to override `self.eval_dataset`. If it is an `datasets.Dataset`, columns not\n",
      " |              accepted by the `model.forward()` method are automatically removed. It must implement the `__len__`\n",
      " |              method.\n",
      " |          ignore_keys (`Lst[str]`, *optional*):\n",
      " |              A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
      " |              gathering predictions.\n",
      " |          metric_key_prefix (`str`, *optional*, defaults to `\"eval\"`):\n",
      " |              An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n",
      " |              \"eval_bleu\" if the prefix is \"eval\" (default)\n",
      " |      \n",
      " |      Returns:\n",
      " |          A dictionary containing the evaluation loss and the potential metrics computed from the predictions. The\n",
      " |          dictionary also contains the epoch number which comes from the training state.\n",
      " |  \n",
      " |  evaluation_loop(self, dataloader: torch.utils.data.dataloader.DataLoader, description: str, prediction_loss_only: Optional[bool] = None, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = 'eval') -> transformers.trainer_utils.EvalLoopOutput\n",
      " |      Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n",
      " |      \n",
      " |      Works both with or without labels.\n",
      " |  \n",
      " |  floating_point_ops(self, inputs: Dict[str, Union[torch.Tensor, Any]])\n",
      " |      For models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point\n",
      " |      operations for every backward + forward pass. If using another model, either implement such a method in the\n",
      " |      model or subclass and override this method.\n",
      " |      \n",
      " |      Args:\n",
      " |          inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
      " |              The inputs and targets of the model.\n",
      " |      \n",
      " |      Returns:\n",
      " |          `int`: The number of floating-point operations.\n",
      " |  \n",
      " |  get_eval_dataloader(self, eval_dataset: Optional[torch.utils.data.dataset.Dataset] = None) -> torch.utils.data.dataloader.DataLoader\n",
      " |      Returns the evaluation [`~torch.utils.data.DataLoader`].\n",
      " |      \n",
      " |      Subclass and override this method if you want to inject some custom behavior.\n",
      " |      \n",
      " |      Args:\n",
      " |          eval_dataset (`torch.utils.data.Dataset`, *optional*):\n",
      " |              If provided, will override `self.eval_dataset`. If it is an `datasets.Dataset`, columns not accepted by\n",
      " |              the `model.forward()` method are automatically removed. It must implement `__len__`.\n",
      " |  \n",
      " |  get_test_dataloader(self, test_dataset: torch.utils.data.dataset.Dataset) -> torch.utils.data.dataloader.DataLoader\n",
      " |      Returns the test [`~torch.utils.data.DataLoader`].\n",
      " |      \n",
      " |      Subclass and override this method if you want to inject some custom behavior.\n",
      " |      \n",
      " |      Args:\n",
      " |          test_dataset (`torch.utils.data.Dataset`, *optional*):\n",
      " |              The test dataset to use. If it is an `datasets.Dataset`, columns not accepted by the `model.forward()`\n",
      " |              method are automatically removed. It must implement `__len__`.\n",
      " |  \n",
      " |  get_train_dataloader(self) -> torch.utils.data.dataloader.DataLoader\n",
      " |      Returns the training [`~torch.utils.data.DataLoader`].\n",
      " |      \n",
      " |      Will use no sampler if `self.train_dataset` does not implement `__len__`, a random sampler (adapted to\n",
      " |      distributed training if necessary) otherwise.\n",
      " |      \n",
      " |      Subclass and override this method if you want to inject some custom behavior.\n",
      " |  \n",
      " |  hyperparameter_search(self, hp_space: Optional[Callable[[ForwardRef('optuna.Trial')], Dict[str, float]]] = None, compute_objective: Optional[Callable[[Dict[str, float]], float]] = None, n_trials: int = 20, direction: str = 'minimize', backend: Union[ForwardRef('str'), transformers.trainer_utils.HPSearchBackend, NoneType] = None, hp_name: Optional[Callable[[ForwardRef('optuna.Trial')], str]] = None, **kwargs) -> transformers.trainer_utils.BestRun\n",
      " |      Launch an hyperparameter search using `optuna` or `Ray Tune` or `SigOpt`. The optimized quantity is determined\n",
      " |      by `compute_objective`, which defaults to a function returning the evaluation loss when no metric is provided,\n",
      " |      the sum of all metrics otherwise.\n",
      " |      \n",
      " |      <Tip warning={true}>\n",
      " |      \n",
      " |      To use this method, you need to have provided a `model_init` when initializing your [`Trainer`]: we need to\n",
      " |      reinitialize the model at each new run. This is incompatible with the `optimizers` argument, so you need to\n",
      " |      subclass [`Trainer`] and override the method [`~Trainer.create_optimizer_and_scheduler`] for custom\n",
      " |      optimizer/scheduler.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Args:\n",
      " |          hp_space (`Callable[[\"optuna.Trial\"], Dict[str, float]]`, *optional*):\n",
      " |              A function that defines the hyperparameter search space. Will default to\n",
      " |              [`~trainer_utils.default_hp_space_optuna`] or [`~trainer_utils.default_hp_space_ray`] or\n",
      " |              [`~trainer_utils.default_hp_space_sigopt`] depending on your backend.\n",
      " |          compute_objective (`Callable[[Dict[str, float]], float]`, *optional*):\n",
      " |              A function computing the objective to minimize or maximize from the metrics returned by the `evaluate`\n",
      " |              method. Will default to [`~trainer_utils.default_compute_objective`].\n",
      " |          n_trials (`int`, *optional*, defaults to 100):\n",
      " |              The number of trial runs to test.\n",
      " |          direction(`str`, *optional*, defaults to `\"minimize\"`):\n",
      " |              Whether to optimize greater or lower objects. Can be `\"minimize\"` or `\"maximize\"`, you should pick\n",
      " |              `\"minimize\"` when optimizing the validation loss, `\"maximize\"` when optimizing one or several metrics.\n",
      " |          backend(`str` or [`~training_utils.HPSearchBackend`], *optional*):\n",
      " |              The backend to use for hyperparameter search. Will default to optuna or Ray Tune or SigOpt, depending\n",
      " |              on which one is installed. If all are installed, will default to optuna.\n",
      " |          kwargs:\n",
      " |              Additional keyword arguments passed along to `optuna.create_study` or `ray.tune.run`. For more\n",
      " |              information see:\n",
      " |      \n",
      " |              - the documentation of\n",
      " |                [optuna.create_study](https://optuna.readthedocs.io/en/stable/reference/generated/optuna.study.create_study.html)\n",
      " |              - the documentation of [tune.run](https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run)\n",
      " |              - the documentation of [sigopt](https://app.sigopt.com/docs/endpoints/experiments/create)\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`trainer_utils.BestRun`]: All the information about the best run.\n",
      " |  \n",
      " |  init_git_repo(self, at_init: bool = False)\n",
      " |      Initializes a git repo in `self.args.hub_model_id`.\n",
      " |      \n",
      " |      Args:\n",
      " |          at_init (`bool`, *optional*, defaults to `False`):\n",
      " |              Whether this function is called before any training or not. If `self.args.overwrite_output_dir` is\n",
      " |              `True` and `at_init` is `True`, the path to the repo (which is `self.args.output_dir`) might be wiped\n",
      " |              out.\n",
      " |  \n",
      " |  is_local_process_zero(self) -> bool\n",
      " |      Whether or not this process is the local (e.g., on one machine if training in a distributed fashion on several\n",
      " |      machines) main process.\n",
      " |  \n",
      " |  is_world_process_zero(self) -> bool\n",
      " |      Whether or not this process is the global main process (when training in a distributed fashion on several\n",
      " |      machines, this is only going to be `True` for one process).\n",
      " |  \n",
      " |  log(self, logs: Dict[str, float]) -> None\n",
      " |      Log `logs` on the various objects watching training.\n",
      " |      \n",
      " |      Subclass and override this method to inject custom behavior.\n",
      " |      \n",
      " |      Args:\n",
      " |          logs (`Dict[str, float]`):\n",
      " |              The values to log.\n",
      " |  \n",
      " |  log_metrics(self, split, metrics)\n",
      " |      Log metrics in a specially formatted way\n",
      " |      \n",
      " |      Under distributed environment this is done only for a process with rank 0.\n",
      " |      \n",
      " |      Args:\n",
      " |          split (`str`):\n",
      " |              Mode/split name: one of `train`, `eval`, `test`\n",
      " |          metrics (`Dict[str, float]`):\n",
      " |              The metrics returned from train/evaluate/predictmetrics: metrics dict\n",
      " |      \n",
      " |      Notes on memory reports:\n",
      " |      \n",
      " |      In order to get memory usage report you need to install `psutil`. You can do that with `pip install psutil`.\n",
      " |      \n",
      " |      Now when this method is run, you will see a report that will include: :\n",
      " |      \n",
      " |      ```\n",
      " |      init_mem_cpu_alloc_delta   =     1301MB\n",
      " |      init_mem_cpu_peaked_delta  =      154MB\n",
      " |      init_mem_gpu_alloc_delta   =      230MB\n",
      " |      init_mem_gpu_peaked_delta  =        0MB\n",
      " |      train_mem_cpu_alloc_delta  =     1345MB\n",
      " |      train_mem_cpu_peaked_delta =        0MB\n",
      " |      train_mem_gpu_alloc_delta  =      693MB\n",
      " |      train_mem_gpu_peaked_delta =        7MB\n",
      " |      ```\n",
      " |      \n",
      " |      **Understanding the reports:**\n",
      " |      \n",
      " |      - the first segment, e.g., `train__`, tells you which stage the metrics are for. Reports starting with `init_`\n",
      " |          will be added to the first stage that gets run. So that if only evaluation is run, the memory usage for the\n",
      " |          `__init__` will be reported along with the `eval_` metrics.\n",
      " |      - the third segment, is either `cpu` or `gpu`, tells you whether it's the general RAM or the gpu0 memory\n",
      " |          metric.\n",
      " |      - `*_alloc_delta` - is the difference in the used/allocated memory counter between the end and the start of the\n",
      " |          stage - it can be negative if a function released more memory than it allocated.\n",
      " |      - `*_peaked_delta` - is any extra memory that was consumed and then freed - relative to the current allocated\n",
      " |          memory counter - it is never negative. When you look at the metrics of any stage you add up `alloc_delta` +\n",
      " |          `peaked_delta` and you know how much memory was needed to complete that stage.\n",
      " |      \n",
      " |      The reporting happens only for process of rank 0 and gpu 0 (if there is a gpu). Typically this is enough since the\n",
      " |      main process does the bulk of work, but it could be not quite so if model parallel is used and then other GPUs may\n",
      " |      use a different amount of gpu memory. This is also not the same under DataParallel where gpu0 may require much more\n",
      " |      memory than the rest since it stores the gradient and optimizer states for all participating GPUS. Perhaps in the\n",
      " |      future these reports will evolve to measure those too.\n",
      " |      \n",
      " |      The CPU RAM metric measures RSS (Resident Set Size) includes both the memory which is unique to the process and the\n",
      " |      memory shared with other processes. It is important to note that it does not include swapped out memory, so the\n",
      " |      reports could be imprecise.\n",
      " |      \n",
      " |      The CPU peak memory is measured using a sampling thread. Due to python's GIL it may miss some of the peak memory if\n",
      " |      that thread didn't get a chance to run when the highest memory was used. Therefore this report can be less than\n",
      " |      reality. Using `tracemalloc` would have reported the exact peak memory, but it doesn't report memory allocations\n",
      " |      outside of python. So if some C++ CUDA extension allocated its own memory it won't be reported. And therefore it\n",
      " |      was dropped in favor of the memory sampling approach, which reads the current process memory usage.\n",
      " |      \n",
      " |      The GPU allocated and peak memory reporting is done with `torch.cuda.memory_allocated()` and\n",
      " |      `torch.cuda.max_memory_allocated()`. This metric reports only \"deltas\" for pytorch-specific allocations, as\n",
      " |      `torch.cuda` memory management system doesn't track any memory allocated outside of pytorch. For example, the very\n",
      " |      first cuda call typically loads CUDA kernels, which may take from 0.5 to 2GB of GPU memory.\n",
      " |      \n",
      " |      Note that this tracker doesn't account for memory allocations outside of [`Trainer`]'s `__init__`, `train`,\n",
      " |      `evaluate` and `predict` calls.\n",
      " |      \n",
      " |      Because `evaluation` calls may happen during `train`, we can't handle nested invocations because\n",
      " |      `torch.cuda.max_memory_allocated` is a single counter, so if it gets reset by a nested eval call, `train`'s tracker\n",
      " |      will report incorrect info. If this [pytorch issue](https://github.com/pytorch/pytorch/issues/16266) gets resolved\n",
      " |      it will be possible to change this class to be re-entrant. Until then we will only track the outer level of\n",
      " |      `train`, `evaluate` and `predict` methods. Which means that if `eval` is called during `train`, it's the latter\n",
      " |      that will account for its memory usage and that of the former.\n",
      " |      \n",
      " |      This also means that if any other tool that is used along the [`Trainer`] calls\n",
      " |      `torch.cuda.reset_peak_memory_stats`, the gpu peak memory stats could be invalid. And the [`Trainer`] will disrupt\n",
      " |      the normal behavior of any such tools that rely on calling `torch.cuda.reset_peak_memory_stats` themselves.\n",
      " |      \n",
      " |      For best performance you may want to consider turning the memory profiling off for production runs.\n",
      " |  \n",
      " |  metrics_format(self, metrics: Dict[str, float]) -> Dict[str, float]\n",
      " |      Reformat Trainer metrics values to a human-readable format\n",
      " |      \n",
      " |      Args:\n",
      " |          metrics (`Dict[str, float]`):\n",
      " |              The metrics returned from train/evaluate/predict\n",
      " |      \n",
      " |      Returns:\n",
      " |          metrics (`Dict[str, float]`): The reformatted metrics\n",
      " |  \n",
      " |  num_examples(self, dataloader: torch.utils.data.dataloader.DataLoader) -> int\n",
      " |      Helper to get number of samples in a [`~torch.utils.data.DataLoader`] by accessing its dataset.\n",
      " |      \n",
      " |      Will raise an exception if the underlying dataset does not implement method `__len__`\n",
      " |  \n",
      " |  pop_callback(self, callback)\n",
      " |      Remove a callback from the current list of [`~transformer.TrainerCallback`] and returns it.\n",
      " |      \n",
      " |      If the callback is not found, returns `None` (and no error is raised).\n",
      " |      \n",
      " |      Args:\n",
      " |         callback (`type` or [`~transformer.TrainerCallback`]):\n",
      " |             A [`~transformer.TrainerCallback`] class or an instance of a [`~transformer.TrainerCallback`]. In the\n",
      " |             first case, will pop the first member of that class found in the list of callbacks.\n",
      " |      \n",
      " |      Returns:\n",
      " |          [`~transformer.TrainerCallback`]: The callback removed, if found.\n",
      " |  \n",
      " |  predict(self, test_dataset: torch.utils.data.dataset.Dataset, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = 'test') -> transformers.trainer_utils.PredictionOutput\n",
      " |      Run prediction and returns predictions and potential metrics.\n",
      " |      \n",
      " |      Depending on the dataset and your use case, your test dataset may contain labels. In that case, this method\n",
      " |      will also return metrics, like in `evaluate()`.\n",
      " |      \n",
      " |      Args:\n",
      " |          test_dataset (`Dataset`):\n",
      " |              Dataset to run the predictions on. If it is an `datasets.Dataset`, columns not accepted by the\n",
      " |              `model.forward()` method are automatically removed. Has to implement the method `__len__`\n",
      " |          ignore_keys (`Lst[str]`, *optional*):\n",
      " |              A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
      " |              gathering predictions.\n",
      " |          metric_key_prefix (`str`, *optional*, defaults to `\"test\"`):\n",
      " |              An optional prefix to be used as the metrics key prefix. For example the metrics \"bleu\" will be named\n",
      " |              \"test_bleu\" if the prefix is \"test\" (default)\n",
      " |      \n",
      " |      <Tip>\n",
      " |      \n",
      " |      If your predictions or labels have different sequence length (for instance because you're doing dynamic padding\n",
      " |      in a token classification task) the predictions will be padded (on the right) to allow for concatenation into\n",
      " |      one array. The padding index is -100.\n",
      " |      \n",
      " |      </Tip>\n",
      " |      \n",
      " |      Returns: *NamedTuple* A namedtuple with the following keys:\n",
      " |      \n",
      " |          - predictions (`np.ndarray`): The predictions on `test_dataset`.\n",
      " |          - label_ids (`np.ndarray`, *optional*): The labels (if the dataset contained some).\n",
      " |          - metrics (`Dict[str, float]`, *optional*): The potential dictionary of metrics (if the dataset contained\n",
      " |            labels).\n",
      " |  \n",
      " |  prediction_loop(self, dataloader: torch.utils.data.dataloader.DataLoader, description: str, prediction_loss_only: Optional[bool] = None, ignore_keys: Optional[List[str]] = None, metric_key_prefix: str = 'eval') -> transformers.trainer_utils.PredictionOutput\n",
      " |      Prediction/evaluation loop, shared by `Trainer.evaluate()` and `Trainer.predict()`.\n",
      " |      \n",
      " |      Works both with or without labels.\n",
      " |  \n",
      " |  prediction_step(self, model: torch.nn.modules.module.Module, inputs: Dict[str, Union[torch.Tensor, Any]], prediction_loss_only: bool, ignore_keys: Optional[List[str]] = None) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]\n",
      " |      Perform an evaluation step on `model` using `inputs`.\n",
      " |      \n",
      " |      Subclass and override to inject custom behavior.\n",
      " |      \n",
      " |      Args:\n",
      " |          model (`nn.Module`):\n",
      " |              The model to evaluate.\n",
      " |          inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
      " |              The inputs and targets of the model.\n",
      " |      \n",
      " |              The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
      " |              argument `labels`. Check your model's documentation for all accepted arguments.\n",
      " |          prediction_loss_only (`bool`):\n",
      " |              Whether or not to return the loss only.\n",
      " |          ignore_keys (`Lst[str]`, *optional*):\n",
      " |              A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
      " |              gathering predictions.\n",
      " |      \n",
      " |      Return:\n",
      " |          Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]: A tuple with the loss,\n",
      " |          logits and labels (each being optional).\n",
      " |  \n",
      " |  push_to_hub(self, commit_message: Optional[str] = 'End of training', blocking: bool = True, **kwargs) -> str\n",
      " |      Upload *self.model* and *self.tokenizer* to the ðŸ¤— model hub on the repo *self.args.hub_model_id*.\n",
      " |      \n",
      " |      Parameters:\n",
      " |          commit_message (`str`, *optional*, defaults to `\"End of training\"`):\n",
      " |              Message to commit while pushing.\n",
      " |          blocking (`bool`, *optional*, defaults to `True`):\n",
      " |              Whether the function should return only when the `git push` has finished.\n",
      " |          kwargs:\n",
      " |              Additional keyword arguments passed along to [`~Trainer.create_model_card`].\n",
      " |      \n",
      " |      Returns:\n",
      " |          The url of the commit of your model in the given repository if `blocking=False`, a tuple with the url of\n",
      " |          the commit and an object to track the progress of the commit if `blocking=True`\n",
      " |  \n",
      " |  remove_callback(self, callback)\n",
      " |      Remove a callback from the current list of [`~transformer.TrainerCallback`].\n",
      " |      \n",
      " |      Args:\n",
      " |         callback (`type` or [`~transformer.TrainerCallback`]):\n",
      " |             A [`~transformer.TrainerCallback`] class or an instance of a [`~transformer.TrainerCallback`]. In the\n",
      " |             first case, will remove the first member of that class found in the list of callbacks.\n",
      " |  \n",
      " |  save_metrics(self, split, metrics, combined=True)\n",
      " |      Save metrics into a json file for that split, e.g. `train_results.json`.\n",
      " |      \n",
      " |      Under distributed environment this is done only for a process with rank 0.\n",
      " |      \n",
      " |      Args:\n",
      " |          split (`str`):\n",
      " |              Mode/split name: one of `train`, `eval`, `test`, `all`\n",
      " |          metrics (`Dict[str, float]`):\n",
      " |              The metrics returned from train/evaluate/predict\n",
      " |          combined (`bool`, *optional*, defaults to `True`):\n",
      " |              Creates combined metrics by updating `all_results.json` with metrics of this call\n",
      " |      \n",
      " |      To understand the metrics please read the docstring of [`~Trainer.log_metrics`]. The only difference is that raw\n",
      " |      unformatted numbers are saved in the current method.\n",
      " |  \n",
      " |  save_model(self, output_dir: Optional[str] = None, _internal_call: bool = False)\n",
      " |      Will save the model, so you can reload it using `from_pretrained()`.\n",
      " |      \n",
      " |      Will only save from the main process.\n",
      " |  \n",
      " |  save_state(self)\n",
      " |      Saves the Trainer state, since Trainer.save_model saves only the tokenizer with the model\n",
      " |      \n",
      " |      Under distributed environment this is done only for a process with rank 0.\n",
      " |  \n",
      " |  store_flos(self)\n",
      " |  \n",
      " |  train(self, resume_from_checkpoint: Union[str, bool, NoneType] = None, trial: Union[ForwardRef('optuna.Trial'), Dict[str, Any]] = None, ignore_keys_for_eval: Optional[List[str]] = None, **kwargs)\n",
      " |      Main training entry point.\n",
      " |      \n",
      " |      Args:\n",
      " |          resume_from_checkpoint (`str` or `bool`, *optional*):\n",
      " |              If a `str`, local path to a saved checkpoint as saved by a previous instance of [`Trainer`]. If a\n",
      " |              `bool` and equals `True`, load the last checkpoint in *args.output_dir* as saved by a previous instance\n",
      " |              of [`Trainer`]. If present, training will resume from the model/optimizer/scheduler states loaded here.\n",
      " |          trial (`optuna.Trial` or `Dict[str, Any]`, *optional*):\n",
      " |              The trial run or the hyperparameter dictionary for hyperparameter search.\n",
      " |          ignore_keys_for_eval (`List[str]`, *optional*)\n",
      " |              A list of keys in the output of your model (if it is a dictionary) that should be ignored when\n",
      " |              gathering predictions for evaluation during the training.\n",
      " |          kwargs:\n",
      " |              Additional keyword arguments used to hide deprecated arguments\n",
      " |  \n",
      " |  training_step(self, model: torch.nn.modules.module.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor\n",
      " |      Perform a training step on a batch of inputs.\n",
      " |      \n",
      " |      Subclass and override to inject custom behavior.\n",
      " |      \n",
      " |      Args:\n",
      " |          model (`nn.Module`):\n",
      " |              The model to train.\n",
      " |          inputs (`Dict[str, Union[torch.Tensor, Any]]`):\n",
      " |              The inputs and targets of the model.\n",
      " |      \n",
      " |              The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
      " |              argument `labels`. Check your model's documentation for all accepted arguments.\n",
      " |      \n",
      " |      Return:\n",
      " |          `torch.Tensor`: The tensor with training loss on this batch.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  get_optimizer_cls_and_kwargs(args: transformers.training_args.TrainingArguments) -> Tuple[Any, Any]\n",
      " |      Returns the optimizer class and optimizer parameters based on the training arguments.\n",
      " |      \n",
      " |      Args:\n",
      " |          args (`transformers.training_args.TrainingArguments`):\n",
      " |              The training arguments for the training session.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3721ffd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2891\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 46\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_46452/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\DLAssignment\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1398\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1399\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1400\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1402\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DLAssignment\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   1982\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1983\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautocast_smart_context_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1984\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1986\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DLAssignment\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2014\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2015\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2016\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2017\u001b[0m         \u001b[1;31m# Save past state if it exists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2018\u001b[0m         \u001b[1;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DLAssignment\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'input_ids'"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7b43162c",
   "metadata": {},
   "outputs": [],
   "source": [
    "te = torch.Tensor([    1,   357,    43,  2977,   530, 23080,    13,    78,    17,     0,\n",
    "         4312,     0,   151,    22, 18215,    17,    17,    46,    43,  2015,\n",
    "            2,  1496,  7369,   115,  4782,    37, 22196,   252, 26998,     0,\n",
    "        28680,     1,   496,  2193,  1037,     9,  4072,   380,    27, 33001,\n",
    "            3,   449,   310,     9,    13,  8034,  3107,   639,    13, 27958,\n",
    "          638,     1,   168,    17,    43,  2786,    15,   160,   152,  3072,\n",
    "            4,  5181, 15182, 18712,   877,    16,   423,    22,   562,  1575,\n",
    "          496,     1,   209,  1056,    17,    39,   317, 19914,   128,   348,\n",
    "            1,    13,    15,    22, 17314,   357,  1517,   209,  2156,   348,\n",
    "          131,  2196,   146, 16561,   188, 31575,   348,    54,    52,   630])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bdf0d6d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 3.5700e+02, 4.3000e+01, 2.9770e+03, 5.3000e+02, 2.3080e+04,\n",
       "         1.3000e+01, 7.8000e+01, 1.7000e+01, 0.0000e+00, 4.3120e+03, 0.0000e+00,\n",
       "         1.5100e+02, 2.2000e+01, 1.8215e+04, 1.7000e+01, 1.7000e+01, 4.6000e+01,\n",
       "         4.3000e+01, 2.0150e+03],\n",
       "        [2.0000e+00, 1.4960e+03, 7.3690e+03, 1.1500e+02, 4.7820e+03, 3.7000e+01,\n",
       "         2.2196e+04, 2.5200e+02, 2.6998e+04, 0.0000e+00, 2.8680e+04, 1.0000e+00,\n",
       "         4.9600e+02, 2.1930e+03, 1.0370e+03, 9.0000e+00, 4.0720e+03, 3.8000e+02,\n",
       "         2.7000e+01, 3.3001e+04],\n",
       "        [3.0000e+00, 4.4900e+02, 3.1000e+02, 9.0000e+00, 1.3000e+01, 8.0340e+03,\n",
       "         3.1070e+03, 6.3900e+02, 1.3000e+01, 2.7958e+04, 6.3800e+02, 1.0000e+00,\n",
       "         1.6800e+02, 1.7000e+01, 4.3000e+01, 2.7860e+03, 1.5000e+01, 1.6000e+02,\n",
       "         1.5200e+02, 3.0720e+03],\n",
       "        [4.0000e+00, 5.1810e+03, 1.5182e+04, 1.8712e+04, 8.7700e+02, 1.6000e+01,\n",
       "         4.2300e+02, 2.2000e+01, 5.6200e+02, 1.5750e+03, 4.9600e+02, 1.0000e+00,\n",
       "         2.0900e+02, 1.0560e+03, 1.7000e+01, 3.9000e+01, 3.1700e+02, 1.9914e+04,\n",
       "         1.2800e+02, 3.4800e+02],\n",
       "        [1.0000e+00, 1.3000e+01, 1.5000e+01, 2.2000e+01, 1.7314e+04, 3.5700e+02,\n",
       "         1.5170e+03, 2.0900e+02, 2.1560e+03, 3.4800e+02, 1.3100e+02, 2.1960e+03,\n",
       "         1.4600e+02, 1.6561e+04, 1.8800e+02, 3.1575e+04, 3.4800e+02, 5.4000e+01,\n",
       "         5.2000e+01, 6.3000e+02]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te = te.view(5,20)\n",
    "te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d5ea1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 3.5700e+02, 4.3000e+01, 2.9770e+03, 5.3000e+02, 2.3080e+04,\n",
       "         1.3000e+01, 7.8000e+01, 1.7000e+01, 0.0000e+00, 4.3120e+03, 0.0000e+00,\n",
       "         1.5100e+02, 2.2000e+01, 1.8215e+04, 1.7000e+01, 1.7000e+01, 4.6000e+01,\n",
       "         4.3000e+01],\n",
       "        [2.0000e+00, 1.4960e+03, 7.3690e+03, 1.1500e+02, 4.7820e+03, 3.7000e+01,\n",
       "         2.2196e+04, 2.5200e+02, 2.6998e+04, 0.0000e+00, 2.8680e+04, 1.0000e+00,\n",
       "         4.9600e+02, 2.1930e+03, 1.0370e+03, 9.0000e+00, 4.0720e+03, 3.8000e+02,\n",
       "         2.7000e+01],\n",
       "        [3.0000e+00, 4.4900e+02, 3.1000e+02, 9.0000e+00, 1.3000e+01, 8.0340e+03,\n",
       "         3.1070e+03, 6.3900e+02, 1.3000e+01, 2.7958e+04, 6.3800e+02, 1.0000e+00,\n",
       "         1.6800e+02, 1.7000e+01, 4.3000e+01, 2.7860e+03, 1.5000e+01, 1.6000e+02,\n",
       "         1.5200e+02],\n",
       "        [4.0000e+00, 5.1810e+03, 1.5182e+04, 1.8712e+04, 8.7700e+02, 1.6000e+01,\n",
       "         4.2300e+02, 2.2000e+01, 5.6200e+02, 1.5750e+03, 4.9600e+02, 1.0000e+00,\n",
       "         2.0900e+02, 1.0560e+03, 1.7000e+01, 3.9000e+01, 3.1700e+02, 1.9914e+04,\n",
       "         1.2800e+02],\n",
       "        [1.0000e+00, 1.3000e+01, 1.5000e+01, 2.2000e+01, 1.7314e+04, 3.5700e+02,\n",
       "         1.5170e+03, 2.0900e+02, 2.1560e+03, 3.4800e+02, 1.3100e+02, 2.1960e+03,\n",
       "         1.4600e+02, 1.6561e+04, 1.8800e+02, 3.1575e+04, 3.4800e+02, 5.4000e+01,\n",
       "         5.2000e+01]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b2f1e8be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 19])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te[:,1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f204aed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0021ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = data.Corpus('./data/wikitext-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b6d4388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    print(data)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz,-1).t().contiguous()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a1aa98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   0,    1,    2,  ..., 1575,  808,  209])\n"
     ]
    }
   ],
   "source": [
    "train_data = batchify(corpus.train, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d903f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   284, 15178,   280,   348,   128,   289,  9493,    16,     1,\n",
       "            13,     0,  2701,  1227,  1563,  4044,   115,  1352,  1335,    16],\n",
       "        [    1,   357,    43,  2977,   530, 23080,    13,    78,    17,     0,\n",
       "          4312,     0,   151,    22, 18215,    17,    17,    46,    43,  2015],\n",
       "        [    2,  1496,  7369,   115,  4782,    37, 22196,   252, 26998,     0,\n",
       "         28680,     1,   496,  2193,  1037,     9,  4072,   380,    27, 33001],\n",
       "        [    3,   449,   310,     9,    13,  8034,  3107,   639,    13, 27958,\n",
       "           638,     1,   168,    17,    43,  2786,    15,   160,   152,  3072],\n",
       "        [    4,  5181, 15182, 18712,   877,    16,   423,    22,   562,  1575,\n",
       "           496,     1,   209,  1056,    17,    39,   317, 19914,   128,   348],\n",
       "        [    1,    13,    15,    22, 17314,   357,  1517,   209,  2156,   348,\n",
       "           131,  2196,   146, 16561,   188, 31575,   348,    54,    52,   630],\n",
       "        [    0,    17,   652, 17400,   115,  3220,    59,    61,    13,   530,\n",
       "         15801,  1110,    22,  3168,   189,    15,    99,  8992,    15,   529],\n",
       "        [    0,  1207, 15183, 18712, 13108,  2004,   271,  1100,    37,    17,\n",
       "            61,    72,  1218,    52,    48,  2853,   530,    13,    61,   496],\n",
       "        [    5,  1870,    13,    93,    37,    13,    13,    16,  2751,  1760,\n",
       "            17,    37,    17,    22, 27572,    17,  5969,  4683, 32643,   530],\n",
       "        [    6,    43,    46,  1775,  4589, 23079,   162,  6659, 26999,    13,\n",
       "          6594,  7781,  4176,  2021,     8, 31575,    22,    46,  3241,  5234]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0765a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(5, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0155af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data,targets = get_batch(train_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "476c3883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,   284, 15178,   280,   348,   128,   289,  9493,    16,     1,\n",
       "            13,     0,  2701,  1227,  1563,  4044,   115,  1352,  1335,    16],\n",
       "        [    1,   357,    43,  2977,   530, 23080,    13,    78,    17,     0,\n",
       "          4312,     0,   151,    22, 18215,    17,    17,    46,    43,  2015],\n",
       "        [    2,  1496,  7369,   115,  4782,    37, 22196,   252, 26998,     0,\n",
       "         28680,     1,   496,  2193,  1037,     9,  4072,   380,    27, 33001],\n",
       "        [    3,   449,   310,     9,    13,  8034,  3107,   639,    13, 27958,\n",
       "           638,     1,   168,    17,    43,  2786,    15,   160,   152,  3072],\n",
       "        [    4,  5181, 15182, 18712,   877,    16,   423,    22,   562,  1575,\n",
       "           496,     1,   209,  1056,    17,    39,   317, 19914,   128,   348]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08773a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,   357,    43,  2977,   530, 23080,    13,    78,    17,     0,\n",
       "         4312,     0,   151,    22, 18215,    17,    17,    46,    43,  2015,\n",
       "            2,  1496,  7369,   115,  4782,    37, 22196,   252, 26998,     0,\n",
       "        28680,     1,   496,  2193,  1037,     9,  4072,   380,    27, 33001,\n",
       "            3,   449,   310,     9,    13,  8034,  3107,   639,    13, 27958,\n",
       "          638,     1,   168,    17,    43,  2786,    15,   160,   152,  3072,\n",
       "            4,  5181, 15182, 18712,   877,    16,   423,    22,   562,  1575,\n",
       "          496,     1,   209,  1056,    17,    39,   317, 19914,   128,   348,\n",
       "            1,    13,    15,    22, 17314,   357,  1517,   209,  2156,   348,\n",
       "          131,  2196,   146, 16561,   188, 31575,   348,    54,    52,   630])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836cb310",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
